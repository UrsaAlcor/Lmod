Bootstrap: localimage
From: Mila.sif
Stage: build


%files
# https://registrationcenter-download.intel.com/akdlm/irc_nas/17977/l_BaseKit_p_2021.3.0.3219_offline.sh
lmod/recipes/l_BaseKit_p_2021.3.0.3219_offline.sh               /opt/
# https://content.mellanox.com/ofed/MLNX_OFED-5.4-1.0.3.0/MLNX_OFED_LINUX-5.4-1.0.3.0-ubuntu18.04-x86_64.tgz
# https://content.mellanox.com/ofed/MLNX_OFED-5.2-2.2.0.0/MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu18.04-x86_64.tgz
#lmod/recipes/MLNX_OFED_LINUX-5.4-1.0.3.0-ubuntu18.04-x86_64.tgz /opt/
lmod/recipes/MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu18.04-x86_64.tgz /opt/
# https://developer.download.nvidia.com/compute/cuda/10.2/Prod/local_installers/cuda_10.2.89_440.33.01_linux.run
lmod/recipes/cuda_10.2.89_440.33.01_linux.run                   /opt/
# https://developer.download.nvidia.com/compute/cuda/11.0.3/local_installers/cuda_11.0.3_450.51.06_linux.run
lmod/recipes/cuda_11.0.3_450.51.06_linux.run                    /opt/
# https://developer.download.nvidia.com/compute/cuda/11.1.1/local_installers/cuda_11.1.1_455.32.00_linux.run
lmod/recipes/cuda_11.1.1_455.32.00_linux.run                    /opt/
# https://developer.download.nvidia.com/compute/cuda/11.2.2/local_installers/cuda_11.2.2_460.32.03_linux.run
lmod/recipes/cuda_11.2.2_460.32.03_linux.run                    /opt/
# https://developer.download.nvidia.com/compute/cuda/11.3.1/local_installers/cuda_11.3.1_465.19.01_linux.run
lmod/recipes/cuda_11.3.1_465.19.01_linux.run                    /opt/
# https://developer.download.nvidia.com/compute/cuda/11.4.0/local_installers/cuda_11.4.0_470.42.01_linux.run
# https://developer.download.nvidia.com/compute/cuda/11.4.1/local_installers/cuda_11.4.1_470.57.02_linux.run
#lmod/recipes/cuda_11.4.0_470.42.01_linux.run                    /opt/
lmod/recipes/cuda_11.4.1_470.57.02_linux.run                    /opt/
# cuDNN (registerware)
lmod/recipes/cudnn-10.2-linux-x64-v8.1.1.33.tgz                 /opt/
lmod/recipes/cudnn-11.2-linux-x64-v8.1.1.33.tgz                 /opt/
# TensorRT (registerware)
lmod/recipes/TensorRT-7.2.3.4.Ubuntu-18.04.x86_64-gnu.cuda-10.2.cudnn8.1.tar.gz  /opt/
lmod/recipes/TensorRT-7.2.3.4.Ubuntu-18.04.x86_64-gnu.cuda-11.0.cudnn8.1.tar.gz  /opt/
lmod/recipes/TensorRT-7.2.3.4.Ubuntu-18.04.x86_64-gnu.cuda-11.1.cudnn8.1.tar.gz  /opt/

# Linker hack
lmod/recipes/ld.lua                                             /usr/bin/x86_64-linux-gnu-ld.lua


%post
apt-get update  -y
apt-get upgrade -y
apt-get install -y python3.6 python3.6-dev python3.6-venv \
                   python3.7 python3.7-dev python3.7-venv \
                   python3.8 python3.8-dev python3.8-venv \
                   python3-setuptools gfortran \
                   build-essential cmake yasm nasm \
                   zip unzip patchelf wget git graphviz \
                   libnuma-dev
cd /opt

#
# - Silence extremely irritating Git "help" message
# - Multi-core control.
# - Increase reproducibility by exporting SOURCE_DATE_EPOCH variable with
#   approximately current timestamp.
#
git config --global advice.detachedHead false
NJ=${NJ:-128}
export SOURCE_DATE_EPOCH=1628563769


# OFED
OFED=MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu18.04-x86_64
tar -xf "$OFED".tgz
cd      "$OFED"
./mlnxofedinstall --force --user-space-only --without-fw-update
cd      ..
rm -Rf  "$OFED" "$OFED".tgz
rm -Rf  mellanox neohost  # Installer left-over junk.

# MKL
bash l_BaseKit_p_2021.3.0.3219_offline.sh -a -s --eula accept \
      --install-dir="/cvmfs/ai.mila.quebec/apps/x86_64/common/oneapi/2021.3.0.3219" \
      --components 'intel.oneapi.lin.mkl.devel:intel.oneapi.lin.dnnl:intel.oneapi.lin.dal.devel:intel.oneapi.lin.dpcpp-cpp-compiler:intel.oneapi.lin.dpcpp_dbg:intel.oneapi.lin.ipp.devel:intel.oneapi.lin.ippcp.devel:intel.oneapi.lin.vpl:intel.oneapi.lin.dpl:intel.oneapi.lin.vtune'
rm   l_BaseKit_p_2021.3.0.3219_offline.sh
rm -Rf intel  # Installer left-over junk.

# CUDA
TK=/cvmfs/ai.mila.quebec/apps/x86_64/common/cuda
mkdir -p "$TK/10.2"
mkdir -p "$TK/11.0"
mkdir -p "$TK/11.1"
mkdir -p "$TK/11.2"
mkdir -p "$TK/11.3"
mkdir -p "$TK/11.4"
sh cuda_10.2.89_*.run --silent --override --no-man-page --toolkit --toolkitpath="$TK/10.2" &
sh cuda_11.0.3_*.run  --silent --override --no-man-page --toolkit --toolkitpath="$TK/11.0" &
sh cuda_11.1.1_*.run  --silent --override --no-man-page --toolkit --toolkitpath="$TK/11.1" &
sh cuda_11.2.2_*.run  --silent --override --no-man-page --toolkit --toolkitpath="$TK/11.2" &
sh cuda_11.3.1_*.run  --silent --override --no-man-page --toolkit --toolkitpath="$TK/11.3" &
sh cuda_11.4.1_*.run  --silent --override --no-man-page --toolkit --toolkitpath="$TK/11.4" &
wait       # Don't erase the installers before they ran!
rm cuda_*

# cuDNN
CUDNN_VERSION_FULL=8.1.1.33
CUDNN_VERSION=${CUDNN_VERSION_FULL%.*}  # Strip sub-patch number
CUDNN_VERSION=${CUDNN_VERSION%.*}       # Strip patch number
CUDNN_PREFIX=/cvmfs/ai.mila.quebec/apps/x86_64/common/cudnn
mkdir -p "$CUDNN_PREFIX/10.2-v${CUDNN_VERSION}"
mkdir -p "$CUDNN_PREFIX/11.2-v${CUDNN_VERSION}"
ln -s 11.2-v${CUDNN_VERSION} "$CUDNN_PREFIX/11.0-v${CUDNN_VERSION}"
ln -s 11.2-v${CUDNN_VERSION} "$CUDNN_PREFIX/11.1-v${CUDNN_VERSION}"
tar --strip-components=1 -xf cudnn-10.2-linux-x64-v${CUDNN_VERSION_FULL}.tgz -C "$CUDNN_PREFIX/10.2-v${CUDNN_VERSION}" &
tar --strip-components=1 -xf cudnn-11.2-linux-x64-v${CUDNN_VERSION_FULL}.tgz -C "$CUDNN_PREFIX/11.2-v${CUDNN_VERSION}" &
wait       # Don't erase the installers before they ran!
rm cudnn-*

# TensorRT
TRT_VERSION_FULL=7.2.3.4
TRT_VERSION=${TRT_VERSION_FULL%.*}  # Strip sub-patch number
TRT_VERSION=${TRT_VERSION%.*}       # Strip patch number
TRT_PREFIX=/cvmfs/ai.mila.quebec/apps/x86_64/common/tensorrt
mkdir -p "$TRT_PREFIX/cuda10.2-cudnn${CUDNN_VERSION}-v${TRT_VERSION}"
mkdir -p "$TRT_PREFIX/cuda11.0-cudnn${CUDNN_VERSION}-v${TRT_VERSION}"
mkdir -p "$TRT_PREFIX/cuda11.1-cudnn${CUDNN_VERSION}-v${TRT_VERSION}"
ln    -s      "cuda11.1-cudnn${CUDNN_VERSION}-v${TRT_VERSION}"  "$TRT_PREFIX/cuda11.2-cudnn${CUDNN_VERSION}-v${TRT_VERSION}"
tar --strip-components=1 -xf TensorRT-${TRT_VERSION_FULL}.Ubuntu-18.04.x86_64-gnu.cuda-10.2.cudnn${CUDNN_VERSION}.tar.gz -C "$TRT_PREFIX/cuda10.2-cudnn${CUDNN_VERSION}-v${TRT_VERSION}" &
tar --strip-components=1 -xf TensorRT-${TRT_VERSION_FULL}.Ubuntu-18.04.x86_64-gnu.cuda-11.0.cudnn${CUDNN_VERSION}.tar.gz -C "$TRT_PREFIX/cuda11.0-cudnn${CUDNN_VERSION}-v${TRT_VERSION}" &
tar --strip-components=1 -xf TensorRT-${TRT_VERSION_FULL}.Ubuntu-18.04.x86_64-gnu.cuda-11.1.cudnn${CUDNN_VERSION}.tar.gz -C "$TRT_PREFIX/cuda11.1-cudnn${CUDNN_VERSION}-v${TRT_VERSION}" &
wait       # Don't erase the installers before they ran!
rm TensorRT-*


#
# LINKER HACK!
#
mv /usr/bin/x86_64-linux-gnu-ld     /usr/bin/x86_64-linux-gnu-ld.orig
mv /usr/bin/x86_64-linux-gnu-ld.lua /usr/bin/x86_64-linux-gnu-ld


#
# LZ4
#
LZ4_VERSION=1.9.3
LZ4_PREFIX="/cvmfs/ai.mila.quebec/apps/x86_64/common/lz4"
git clone 'https://github.com/lz4/lz4.git' && cd lz4
git checkout v$LZ4_VERSION
make install -j$NJ MOREFLAGS="-mtune=haswell -Wl,-z,now" \
                   PREFIX="$LZ4_PREFIX/$LZ4_VERSION"
cd ..
rm -Rf lz4


#
# LZMA/XZ-Utils
#
LZMA_VERSION=5.2.5
LZMA_PREFIX="/cvmfs/ai.mila.quebec/apps/x86_64/common/lzma"
wget "https://tukaani.org/xz/xz-${LZMA_VERSION}.tar.gz"
tar -xf xz-${LZMA_VERSION}.tar.gz && cd xz-${LZMA_VERSION}
./configure --prefix="$LZMA_PREFIX/$LZMA_VERSION"
make install -j$NJ
cd  ..
rm  -Rf xz-${LZMA_VERSION}.tar.gz xz-${LZMA_VERSION}


#
# Zstd
#
ZSTD_VERSION=1.5.0
ZSTD_PREFIX="/cvmfs/ai.mila.quebec/apps/x86_64/common/zstd"
git clone 'https://github.com/facebook/zstd.git' && cd zstd
git checkout v${ZSTD_VERSION}
env LIBRARY_PATH="$LZ4_PREFIX/$LZ4_VERSION/lib:$LZMA_PREFIX/$LZMA_VERSION/lib" \
    CPATH="$LZ4_PREFIX/$LZ4_VERSION/include:$LZMA_PREFIX/$LZMA_VERSION/include" \
    make install -j$NJ MOREFLAGS="-mtune=haswell -Wl,-z,now" \
                       PREFIX="$ZSTD_PREFIX/$ZSTD_VERSION"
cd ..
rm -Rf zstd


#
# Py-LMDB
#
# py-lmdb uses a patched, statically-linked version of LMDB, but generally a
# py-lmdb version bundles a specific matching LMDB version.
#
# Very handily, the installed source code for py-lmdb is exactly identical
# across all Python versions except for the native C extensions, which all
# happen to have a versioned ABI tag. Therefore, it is possible to install
# py-lmdb for all Python 3.x versions in "overlapped" fashion, thus having
# a common PYTHONPATH for each.
#
LMDB_VERSION=0.9.29
LMDB_PREFIX="/cvmfs/ai.mila.quebec/apps/x86_64/common/lmdb"
git clone 'https://github.com/LMDB/lmdb.git' && cd lmdb/libraries/liblmdb
git checkout LMDB_${LMDB_VERSION}
make install -j$NJ XCFLAGS="-mtune=haswell -Wl,-z,now" \
                   prefix="$LMDB_PREFIX/$LMDB_VERSION"
cd ../../..
rm -Rf lmdb

PYLMDB_VERSION=1.2.1
PYLMDB_PREFIX="/cvmfs/ai.mila.quebec/apps/x86_64/common/py-lmdb"
git clone 'https://github.com/jnwatson/py-lmdb.git' && cd py-lmdb
git checkout py-lmdb_${PYLMDB_VERSION}
for PYV in 3.6 3.7 3.8; do
  python${PYV} -m venv venv${PYV}
  (
    . venv${PYV}/bin/activate
    python -m pip install wheel Cython
    env -u LMDB_FORCE_CFFI   \
        -u LMDB_FORCE_SYSTEM \
        -u LMDB_PURE         \
        -u LMDB_INCLUDEDIR   \
        -u LMDB_LIBDIR       \
        CFLAGS=" -mtune=haswell -Wl,-z,now" \
        LDFLAGS="-mtune=haswell -Wl,-z,now" \
        python setup.py bdist_wheel
    deactivate
  )
  mkdir -p  "$PYLMDB_PREFIX/$PYLMDB_VERSION"
  unzip -od "$PYLMDB_PREFIX/$PYLMDB_VERSION" dist/lmdb-*.whl
  git clean -xdff
done
cd ..
rm -Rf py-lmdb


#
# OpenBLAS
# The target list on x86_64 is
#     SSE2:     GENERIC      (x86-64-baseline)
#     SSE3:     Prescott
#     SSE4.2:   Nehalem      (x86-64-v2)
#     AVX:      Sandy Bridge
#     AVX2+FMA: Haswell      (x86-64-v3)
#     AVX512:   Skylake-X    (x86-64-v4)
#     AMD AVX2: Zen2         (x86-64-v3, ~Haswell)
#
OPENBLAS_VERSION=0.3.17
OPENBLAS_PREFIX="/cvmfs/ai.mila.quebec/apps/x86_64/common/openblas"
git clone 'https://github.com/xianyi/OpenBLAS.git' && cd OpenBLAS
git checkout v${OPENBLAS_VERSION}
make -j$NJ   USE_THREAD=1 USE_OPENMP=0 NUM_THREADS=256 BUILD_BFLOAT16=1 TARGET=GENERIC \
             DYNAMIC_ARCH=1 DYNAMIC_LIST="PRESCOTT NEHALEM SANDYBRIDGE HASWELL SKYLAKEX ZEN"
make install PREFIX="$OPENBLAS_PREFIX/$OPENBLAS_VERSION"
rmdir "$OPENBLAS_PREFIX/$OPENBLAS_VERSION/bin" || true  # Usually empty.
cd ..
rm -Rf OpenBLAS


#
# Numpy
# We link against OpenBLAS, and build for the following combinations:
#   - v1.19.5: py3.6+
#   - v1.20.3: py3.7+
#   - v1.21.1: py3.7+
# Numpy's setup.py seems to ignore CPATH and LIBRARY_PATH; It seems to only
# care about
#   - OPENBLAS (for OpenBLAS) or
#   - MKLROOT  (for MKL)
# to find the location of the library.
#
NPY_PREFIX="/cvmfs/ai.mila.quebec/apps/x86_64/common/numpy"
export OPENBLAS="$OPENBLAS_PREFIX/$OPENBLAS_VERSION"
export NPY_BLAS_ORDER="OpenBLAS"
export NPY_LAPACK_ORDER="OpenBLAS"
export NPY_NUM_BUILD_JOBS=$NJ
git clone https://github.com/numpy/numpy.git Numpy
for NPY in 1.19.5 1.20.3 1.21.1; do
  if [ "$NPY" = 1.19.5 ]; then PYV_LIST="3.6 3.7 3.8" CPU_BASELINE=''
  else                         PYV_LIST="    3.7 3.8" CPU_BASELINE='--cpu-baseline=sse sse2'
  fi
  for PYV in $PYV_LIST; do
    PYVS="$(printf '%s\n' "$PYV" | sed 's/\.//g')"
    cp -a Numpy "Numpy-${SPY}-cp${PYVS}"
    cd          "Numpy-${SPY}-cp${PYVS}"
    python${PYV} -m venv venv
    git checkout v$NPY
    (
      . venv/bin/activate
      python -m pip install wheel Cython
      python setup.py build ${CPU_BASELINE:+"$CPU_BASELINE"} bdist_wheel
      deactivate
      mkdir -p "${NPY_PREFIX}/${NPY}-cp${PYVS}"
      unzip -d "${NPY_PREFIX}/${NPY}-cp${PYVS}" dist/numpy-*.whl
    ) &
    cd ..
  done
done
wait
rm -Rf Numpy Numpy-*
NPY_OLD=1.19.5   # Used as compatibility target in further recipes!


#
# Scipy
# The BLAS configuration uses the same mechanism as Numpy, but Scipy also
# depends on a package called Pythran.
#
# We deliberately build against the oldest Numpy from above.
#
SPY_PREFIX="/cvmfs/ai.mila.quebec/apps/x86_64/common/scipy"
git clone https://github.com/scipy/scipy.git Scipy
for SPY in 1.5.4 1.6.3 1.7.1; do
  if [ "$SPY" = 1.5.4 ]; then PYV_LIST="3.6 3.7 3.8"
  else                        PYV_LIST="    3.7 3.8"
  fi
  for PYV in $PYV_LIST; do
    PYVS="$(printf '%s\n' "$PYV" | sed 's/\.//g')"
    cp -a Scipy "Scipy-${SPY}-cp${PYVS}"
    cd          "Scipy-${SPY}-cp${PYVS}"
    python${PYV} -m venv venv
    git checkout v$SPY
    git submodule sync
    git submodule update --init
    (
      . venv/bin/activate
      export PYTHONPATH="${NPY_PREFIX}/${NPY_OLD}-cp${PYVS}:${PYTHONPATH}"
      python -m pip install wheel Cython pythran pybind11
      python setup.py bdist_wheel
      deactivate
      mkdir -p "${SPY_PREFIX}/${SPY}-cp${PYVS}"
      unzip -d "${SPY_PREFIX}/${SPY}-cp${PYVS}" dist/scipy-*.whl
    ) &
    cd ..
  done
done
wait
rm -Rf Scipy Scipy-*
SPY_OLD=1.5.4   # Used as compatibility target in further recipes!


#
# Virtualenv Cleanup
#
rm -Rf /opt/venv*


#
# CUDA-related
# We support:
#   - Kepler  (K80 only, legacy)
#   - Maxwell (M40 only, legacy)
#   - Volta   (V100)
#   - Turing  (RTX8000, TITAN RTX)
# And on CUDA Toolkits where they are supported,
#   - Ampere  (A100, A6000)
#

#
# NCCL
#
NCCL_VERSION_BUILD=2.10.3-1
NCCL_VERSION=${NCCL_VERSION_BUILD%%-*}   # Strip build number
NCCL_VERSION=${NCCL_VERSION%.*}          # Strip patch number
NCCL_PREFIX="/cvmfs/ai.mila.quebec/apps/x86_64/common/nccl"
git clone https://github.com/NVIDIA/nccl.git NCCL && cd NCCL
git checkout v$NCCL_VERSION_BUILD
for CUDA in 10.2 11.0 11.1 11.2 11.3 11.4; do
  case "$CUDA" in
    10.2 )
      NVCC_GENCODE="-gencode=arch=compute_37,code=sm_37 -gencode=arch=compute_52,code=sm_52 \
                    -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 \
                    -Wno-deprecated-gpu-targets -Xfatbin -compress-all"
      ;;
    11.0 )
      NVCC_GENCODE="-gencode=arch=compute_37,code=sm_37 -gencode=arch=compute_52,code=sm_52 \
                    -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 \
                    -gencode=arch=compute_80,code=sm_80 \
                    -Wno-deprecated-gpu-targets -Xfatbin -compress-all"
      ;;
    11.[^0] )
      NVCC_GENCODE="-gencode=arch=compute_37,code=sm_37 -gencode=arch=compute_52,code=sm_52 \
                    -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 \
                    -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 \
                    -Wno-deprecated-gpu-targets -Xfatbin -compress-all"
      ;;
  esac
  (
    export PATH="$TK/$CUDA/bin:${PATH}"
    export CPATH="$TK/$CUDA/include"
    export LIBRARY_PATH="$TK/$CUDA/lib64"
    export LDFLAGS="-Wl,-z,now,-rpath,$TK/$CUDA/lib64"
    make -j$NJ src.build CUDA_HOME="$TK/$CUDA" NVCC_GENCODE="$NVCC_GENCODE"
    make install PREFIX="$NCCL_PREFIX/$CUDA-v$NCCL_VERSION"
    make clean
  )
  git clean -xdff
done
cd ..
rm -Rf NCCL


#
# MAGMA
#
export MAGMA_VERSION=2.6.1
export MAGMA_PREFIX="/cvmfs/ai.mila.quebec/apps/x86_64/common/magma"
git clone https://bitbucket.org/icl/magma.git MAGMA && cd MAGMA
git checkout v${MAGMA_VERSION}
for CUDA in 10.2 11.0 11.1 11.2 11.3 11.4; do
  case "$CUDA" in
    10.2 )     GPU_TARGET="sm_37 sm_52 sm_70 sm_75" ;;
    11.0 )     GPU_TARGET="sm_37 sm_52 sm_70 sm_75 sm_80" ;;
    11.[^0] )  GPU_TARGET="sm_37 sm_52 sm_70 sm_75 sm_80 sm_86" ;;
  esac
  #
  # Lightly edit the example OpenBLAS makefile as it is copied to make it more usable:
  #   - Enable overriding GPU_TARGET
  #   - Disable OpenMP (bad forking with GNU OMP libgomp)
  #
  sed -e 's/GPU_TARGET\s*=/GPU_TARGET ?=/g' \
      -e 's/-fopenmp//g' \
      make.inc-examples/make.inc.openblas > make.inc
  env BACKEND=cuda \
      PATH="$TK/$CUDA/bin:${PATH}" \
      CUDADIR="$TK/$CUDA" \
      LIBDIR="-L$TK/$CUDA/lib64" \
      OPENBLASDIR="$OPENBLAS_PREFIX/$OPENBLAS_VERSION" \
      GPU_TARGET="$GPU_TARGET" \
      NVCCFLAGS="-Wno-deprecated-gpu-targets -Xfatbin -compress-all" \
      make install -j$NJ prefix="$MAGMA_PREFIX/$CUDA-v$MAGMA_VERSION"
  git clean -xdff
done
cd ..
rm -Rf MAGMA


#
# PyTorch
#
PYTORCH_PREFIX="/cvmfs/ai.mila.quebec/apps/x86_64/common/pytorch"
git clone --recursive https://github.com/pytorch/pytorch.git PyTorch
for PYTORCH_VERSION in 1.9.0 1.8.1; do
  cd PyTorch
  git clean -xdff
  git checkout v${PYTORCH_VERSION}
  git submodule sync --recursive
  git submodule update --init --recursive
  git submodule sync --recursive
  git submodule update --init --recursive
  git clean -xdff
  cd ..
  for CUDA in 10.2 11.0 11.1 11.2; do
    case "$CUDA" in
      10.2 )     TORCH_CUDA_ARCH_LIST="3.7;5.2;7.0;7.5" ;;
      11.0 )     TORCH_CUDA_ARCH_LIST="3.7;5.2;7.0;7.5;8.0" ;;
      11.[^0] )  TORCH_CUDA_ARCH_LIST="3.7;5.2;7.0;7.5;8.0;8.6" ;;
    esac
    for PYV in 3.6 3.7 3.8; do
      PYVS="$(printf '%s\n' "$PYV" | sed 's/\.//g')"
      cp -a PyTorch "PyTorch-${PYVS}"
      (
        cd          "PyTorch-${PYVS}"
        python${PYV} -m venv venv
        . venv/bin/activate
        
        export PYTHONPATH="${SPY_PREFIX}/${SPY_OLD}-cp${PYVS}:${PYTHONPATH}"
        export PYTHONPATH="${NPY_PREFIX}/${NPY_OLD}-cp${PYVS}:${PYTHONPATH}"
        
        export PYTORCH_BUILD_VERSION="${PYTORCH_VERSION}"
        export PYTORCH_BUILD_NUMBER=0
        export TORCH_NVCC_FLAGS="-Wno-deprecated-gpu-targets -Xfatbin -compress-all"
        export TORCH_CUDA_ARCH_LIST
        export BLAS=OpenBLAS             # Require OpenBLAS for linear algebra. Our OpenBLAS is built against pthreads and not OpenMP.
        export ATEN_THREADING=TBB        # Require TBB rather than OpenMP because OpenMP forks badly.
        export MKL_THREADING=TBB         # Require TBB rather than OpenMP because OpenMP forks badly.
        export MKLDNN_CPU_RUNTIME=TBB    # Require TBB rather than OpenMP because OpenMP forks badly.
        export MKLDNN_ARCH_OPT_FLAGS="-msse2"  # Require portable MKLDNN (SSE2, ddefaults to SSE4.1)
        export DNNL_ARCH_OPT_FLAGS="-msse2"    # Require portable MKLDNN (SSE2, ddefaults to SSE4.1)
        export USE_TBB=1                 # Require TBB
        export USE_OPENMP=0              # Demand not to use OpenMP (due to fork-unsafeness of libgomp, the default implementation of OpenMP).
        export USE_MKLDNN=1              # Demand to use MKLDNN
        export USE_CUDNN=1               # Demand to use cuDNN
        export USE_STATIC_CUDNN=0        # Want dynamic link to avoid wasting disk space / static link of cuDNN so users don't have to install themselves.
        export USE_SYSTEM_NCCL=1         # Compile using system NCCL
        export USE_STATIC_NCCL=0         # Want dynamic link to avoid wasting disk space / static link of NCCL so users don't have to install themselves.
        export USE_DISTRIBUTED=1         # Want distributed backends enabled
        export USE_MPI=1                 # Want (Open)MPI  distributed backend
        export USE_GLOO=1                # Want Gloo       distributed backend
        export USE_TENSORPIPE=1          # Want TensorPipe distributed backend
        export USE_IBVERBS=1             # Want ibverbs
        export USE_ZSTD=1                # Want Zstd
        export USE_LMDB=1                # Want LMDB
        export USE_TENSORRT=1            # Want TensorRT
        export BUILD_CUSTOM_PROTOBUF=1   # Enable building custom protobuf.
        
        export CUDA_HOME="$TK/$CUDA"                                    # The standard way to signal the CUDA Toolkit's location.
        export CUDNN_ROOT="$CUDNN_PREFIX/${CUDA}-v${CUDNN_VERSION}"     # Wanted by FindCUDNN.cmake
        export NCCL_VERSION                                             # Wanted by FindNCCL.cmake, forces correct version check.
        export NCCL_ROOT="$NCCL_PREFIX/${CUDA}-v${NCCL_VERSION}"        # Wanted by FindNCCL.cmake
        export NCCL_LIB_DIR="${NCCL_ROOT}/lib"                          # Wanted by FindNCCL.cmake
        export NCCL_INCLUDE_DIR="${NCCL_ROOT}/include"                  # Wanted by FindNCCL.cmake
        export TENSORRT_ROOT="$TRT_PREFIX/cuda${CUDA}-cudnn${CUDNN_VERSION}-v${TRT_VERSION}"  # Wanted by cmake/public/cuda.cmake
        export TENSORRT_INCLUDE_DIR="${TENSORRT_ROOT}/include"          # Wanted by cmake/public/cuda.cmake
        export LMDB_DIR="$LMDB_PREFIX/$LMDB_VERSION"                    # Wanted by FindLMDB.cmake
        export MAGMA_HOME="$MAGMA_PREFIX/${CUDA}-v${MAGMA_VERSION}"     # Wanted by FindMAGMA.cmake
        export OpenBLAS_HOME="$OPENBLAS_PREFIX/$OPENBLAS_VERSION"       # Wanted by FindOpenBLAS.cmake (PyTorch 1.8.x)
        export LIBRARY_PATH="$OPENBLAS_PREFIX/$OPENBLAS_VERSION/lib"    # Wanted by FindBLAS.cmake     (PyTorch 1.9.x)
        export CPATH="$OPENBLAS_PREFIX/$OPENBLAS_VERSION/include"       # Wanted by FindBLAS.cmake     (PyTorch 1.9.x)
        export PATH="/usr/mpi/gcc/openmpi-4.1.0rc5/bin:$PATH"           # Sufficient to discover OpenMPI
        
        # Propagate additional variables into CMake's invocation
        sed -i -e "s:'MKLDNN_CPU_RUNTIME',:'MKLDNN_CPU_RUNTIME', 'MKLDNN_ARCH_OPT_FLAGS', 'DNNL_ARCH_OPT_FLAGS', 'TENSORRT_ROOT', 'SOURCE_DATE_EPOCH',:g" tools/setup_helpers/cmake.py
        
        # Install wheel first (becauses the requirements need it), then reqs, then build PyTorch.
        python -m pip install wheel
        python -m pip install -r requirements.txt
        python setup.py bdist_wheel
        
        # Repair patch to tools/setup_helpers/cmake.py
        git checkout -- tools/setup_helpers/cmake.py
        
        # Expand the wheel into the destination.
        mkdir -p "${PYTORCH_PREFIX}/cuda${CUDA}-cudnn${CUDNN_VERSION}-v${PYTORCH_VERSION}-cp${PYVS}"
        unzip -d "${PYTORCH_PREFIX}/cuda${CUDA}-cudnn${CUDNN_VERSION}-v${PYTORCH_VERSION}-cp${PYVS}" dist/torch-*.whl
        
        # Leave
        deactivate
        cd ..
        rm -Rf "PyTorch-${PYVS}"
      ) &
    done
    wait
  done
done
cd ..
rm -Rf PyTorch
