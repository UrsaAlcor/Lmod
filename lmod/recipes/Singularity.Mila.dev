Bootstrap: localimage
From: Mila.sif
Stage: build


%files
# https://registrationcenter-download.intel.com/akdlm/irc_nas/17977/l_BaseKit_p_2021.3.0.3219_offline.sh
lmod/recipes/l_BaseKit_p_2021.3.0.3219_offline.sh               /opt/
# https://content.mellanox.com/ofed/MLNX_OFED-5.4-1.0.3.0/MLNX_OFED_LINUX-5.4-1.0.3.0-ubuntu18.04-x86_64.tgz
# https://content.mellanox.com/ofed/MLNX_OFED-5.2-2.2.0.0/MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu18.04-x86_64.tgz
#lmod/recipes/MLNX_OFED_LINUX-5.4-1.0.3.0-ubuntu18.04-x86_64.tgz /opt/
lmod/recipes/MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu18.04-x86_64.tgz /opt/
# https://developer.download.nvidia.com/compute/cuda/10.2/Prod/local_installers/cuda_10.2.89_440.33.01_linux.run
lmod/recipes/cuda_10.2.89_440.33.01_linux.run                   /opt/
# https://developer.download.nvidia.com/compute/cuda/11.0.3/local_installers/cuda_11.0.3_450.51.06_linux.run
lmod/recipes/cuda_11.0.3_450.51.06_linux.run                    /opt/
# https://developer.download.nvidia.com/compute/cuda/11.1.1/local_installers/cuda_11.1.1_455.32.00_linux.run
lmod/recipes/cuda_11.1.1_455.32.00_linux.run                    /opt/
# https://developer.download.nvidia.com/compute/cuda/11.2.2/local_installers/cuda_11.2.2_460.32.03_linux.run
lmod/recipes/cuda_11.2.2_460.32.03_linux.run                    /opt/
# https://developer.download.nvidia.com/compute/cuda/11.3.1/local_installers/cuda_11.3.1_465.19.01_linux.run
lmod/recipes/cuda_11.3.1_465.19.01_linux.run                    /opt/
# https://developer.download.nvidia.com/compute/cuda/11.4.0/local_installers/cuda_11.4.0_470.42.01_linux.run
# https://developer.download.nvidia.com/compute/cuda/11.4.1/local_installers/cuda_11.4.1_470.57.02_linux.run
#lmod/recipes/cuda_11.4.0_470.42.01_linux.run                    /opt/
lmod/recipes/cuda_11.4.1_470.57.02_linux.run                    /opt/
# cuDNN (registerware)
lmod/recipes/cudnn-10.2-linux-x64-v8.1.1.33.tgz                 /opt/
lmod/recipes/cudnn-11.2-linux-x64-v8.1.1.33.tgz                 /opt/
# TensorRT (registerware)
lmod/recipes/TensorRT-7.2.3.4.Ubuntu-18.04.x86_64-gnu.cuda-10.2.cudnn8.1.tar.gz  /opt/
lmod/recipes/TensorRT-7.2.3.4.Ubuntu-18.04.x86_64-gnu.cuda-11.0.cudnn8.1.tar.gz  /opt/
lmod/recipes/TensorRT-7.2.3.4.Ubuntu-18.04.x86_64-gnu.cuda-11.1.cudnn8.1.tar.gz  /opt/

# Linker hack
lmod/recipes/ld.lua                                             /usr/bin/x86_64-linux-gnu-ld.lua


%post
apt-get update  -y
apt-get upgrade -y
apt-get install -y python3.6 python3.6-dev python3.6-venv \
                   python3.7 python3.7-dev python3.7-venv \
                   python3.8 python3.8-dev python3.8-venv \
                   python3-setuptools gfortran \
                   build-essential ccache cmake yasm nasm \
                   zip unzip patchelf wget git graphviz \
                   libnuma-dev
cd /opt

#
# - Silence extremely irritating Git "help" message
# - Multi-core control.
# - Increase reproducibility by exporting SOURCE_DATE_EPOCH variable with
#   approximately current timestamp.
# - Remove limits on compiler caching
#
git config --global advice.detachedHead false
NJ=${NJ:-128}
export SOURCE_DATE_EPOCH=1629769335
ccache -M 0   # Set unlimited size  of compiler cache
ccache -F 0   # Set unlimited files in compiler cache
ccache -z     # Zero  statistics    of compiler cache
ccache -s     # Print statistics    of compiler cache


# OFED
OFED=MLNX_OFED_LINUX-5.2-2.2.0.0-ubuntu18.04-x86_64
if [ ! -d "/usr/mpi/gcc/openmpi-4.1.0rc5" ]; then
  tar -xf "$OFED".tgz
  cd      "$OFED"
  ./mlnxofedinstall --force --user-space-only --without-fw-update
  cd      ..
fi
rm -Rf mellanox neohost "$OFED" "$OFED".tgz # Installer left-over junk.

# MKL
if [ ! -d "/cvmfs/ai.mila.quebec/apps/x86_64/common/oneapi/2021.3.0.3219" ]; then
  bash  l_BaseKit_p_2021.3.0.3219_offline.sh -a -s --eula accept \
        --install-dir="/cvmfs/ai.mila.quebec/apps/x86_64/common/oneapi/2021.3.0.3219" \
        --components 'intel.oneapi.lin.mkl.devel:intel.oneapi.lin.dnnl:intel.oneapi.lin.dal.devel:intel.oneapi.lin.dpcpp-cpp-compiler:intel.oneapi.lin.dpcpp_dbg:intel.oneapi.lin.ipp.devel:intel.oneapi.lin.ippcp.devel:intel.oneapi.lin.vpl:intel.oneapi.lin.dpl:intel.oneapi.lin.vtune'
fi
rm -Rf intel l_BaseKit_p_2021.3.0.3219_offline.sh # Installer left-over junk.

# CUDA
TK=/cvmfs/ai.mila.quebec/apps/x86_64/common/cuda
for CUDA in 10.2 11.0 11.1 11.2 11.3 11.4; do
  if [ ! -d "$TK/$CUDA" ]; then
    (
      rm -Rf   "$TK/$CUDA"
      mkdir -p "$TK/$CUDA"
      sh cuda_${CUDA}.*.run --silent --override --no-man-page --toolkit --toolkitpath="$TK/$CUDA"
      rm cuda_${CUDA}.*.run
    ) &
  fi
done
wait
rm -f cuda_*.run

# cuDNN
CUDNN_VERSION_FULL=8.1.1.33
CUDNN_VERSION=${CUDNN_VERSION_FULL%.*}  # Strip sub-patch number
CUDNN_VERSION=${CUDNN_VERSION%.*}       # Strip patch number
CUDNN_PREFIX=/cvmfs/ai.mila.quebec/apps/x86_64/common/cudnn
if [ -f cudnn-10.2-linux-x64-v${CUDNN_VERSION_FULL}.tgz -a \
     -f cudnn-11.2-linux-x64-v${CUDNN_VERSION_FULL}.tgz ]; then
  rm -Rf   "$CUDNN_PREFIX/10.2-v${CUDNN_VERSION}"
  rm -Rf   "$CUDNN_PREFIX/11.0-v${CUDNN_VERSION}"
  rm -Rf   "$CUDNN_PREFIX/11.1-v${CUDNN_VERSION}"
  rm -Rf   "$CUDNN_PREFIX/11.2-v${CUDNN_VERSION}"
  mkdir -p "$CUDNN_PREFIX/10.2-v${CUDNN_VERSION}"
  mkdir -p "$CUDNN_PREFIX/11.2-v${CUDNN_VERSION}"
  ln -s 11.2-v${CUDNN_VERSION} "$CUDNN_PREFIX/11.0-v${CUDNN_VERSION}"
  ln -s 11.2-v${CUDNN_VERSION} "$CUDNN_PREFIX/11.1-v${CUDNN_VERSION}"
  tar --strip-components=1 -xf cudnn-10.2-linux-x64-v${CUDNN_VERSION_FULL}.tgz -C "$CUDNN_PREFIX/10.2-v${CUDNN_VERSION}" &
  tar --strip-components=1 -xf cudnn-11.2-linux-x64-v${CUDNN_VERSION_FULL}.tgz -C "$CUDNN_PREFIX/11.2-v${CUDNN_VERSION}" &
  wait       # Don't erase the installers before they ran!
  rm cudnn-*
fi

# TensorRT
TRT_VERSION_FULL=7.2.3.4
TRT_VERSION=${TRT_VERSION_FULL%.*}  # Strip sub-patch number
TRT_VERSION=${TRT_VERSION%.*}       # Strip patch number
TRT_PREFIX=/cvmfs/ai.mila.quebec/apps/x86_64/common/tensorrt
if [ -f TensorRT-${TRT_VERSION_FULL}.Ubuntu-18.04.x86_64-gnu.cuda-10.2.cudnn${CUDNN_VERSION}.tar.gz -a \
     -f TensorRT-${TRT_VERSION_FULL}.Ubuntu-18.04.x86_64-gnu.cuda-11.0.cudnn${CUDNN_VERSION}.tar.gz -a \
     -f TensorRT-${TRT_VERSION_FULL}.Ubuntu-18.04.x86_64-gnu.cuda-11.1.cudnn${CUDNN_VERSION}.tar.gz ]; then
  rm -Rf   "$TRT_PREFIX/cuda10.2-cudnn${CUDNN_VERSION}-v${TRT_VERSION}"
  rm -Rf   "$TRT_PREFIX/cuda11.0-cudnn${CUDNN_VERSION}-v${TRT_VERSION}"
  rm -Rf   "$TRT_PREFIX/cuda11.1-cudnn${CUDNN_VERSION}-v${TRT_VERSION}"
  rm -Rf   "$TRT_PREFIX/cuda11.2-cudnn${CUDNN_VERSION}-v${TRT_VERSION}"
  mkdir -p "$TRT_PREFIX/cuda10.2-cudnn${CUDNN_VERSION}-v${TRT_VERSION}"
  mkdir -p "$TRT_PREFIX/cuda11.0-cudnn${CUDNN_VERSION}-v${TRT_VERSION}"
  mkdir -p "$TRT_PREFIX/cuda11.1-cudnn${CUDNN_VERSION}-v${TRT_VERSION}"
  ln    -s "cuda11.1-cudnn${CUDNN_VERSION}-v${TRT_VERSION}"  "$TRT_PREFIX/cuda11.2-cudnn${CUDNN_VERSION}-v${TRT_VERSION}"
  tar --strip-components=1 -xf TensorRT-${TRT_VERSION_FULL}.Ubuntu-18.04.x86_64-gnu.cuda-10.2.cudnn${CUDNN_VERSION}.tar.gz -C "$TRT_PREFIX/cuda10.2-cudnn${CUDNN_VERSION}-v${TRT_VERSION}" &
  tar --strip-components=1 -xf TensorRT-${TRT_VERSION_FULL}.Ubuntu-18.04.x86_64-gnu.cuda-11.0.cudnn${CUDNN_VERSION}.tar.gz -C "$TRT_PREFIX/cuda11.0-cudnn${CUDNN_VERSION}-v${TRT_VERSION}" &
  tar --strip-components=1 -xf TensorRT-${TRT_VERSION_FULL}.Ubuntu-18.04.x86_64-gnu.cuda-11.1.cudnn${CUDNN_VERSION}.tar.gz -C "$TRT_PREFIX/cuda11.1-cudnn${CUDNN_VERSION}-v${TRT_VERSION}" &
  wait       # Don't erase the installers before they ran!
  ln    -s "$TK/10.2/lib64/libnvrtc.so.10.2" "$TRT_PREFIX/cuda10.2-cudnn${CUDNN_VERSION}-v${TRT_VERSION}/lib/libnvrtc.so.10.2"  # Required because libnvinfer.so.7 depends on libnvrtc.so.10.2
  ln    -s "$TK/11.0/lib64/libnvrtc.so.11.0" "$TRT_PREFIX/cuda11.0-cudnn${CUDNN_VERSION}-v${TRT_VERSION}/lib/libnvrtc.so.11.0"  # Required because libnvinfer.so.7 depends on libnvrtc.so.11.0
  ln    -s "$TK/11.1/lib64/libnvrtc.so.11.1" "$TRT_PREFIX/cuda11.1-cudnn${CUDNN_VERSION}-v${TRT_VERSION}/lib/libnvrtc.so.11.1"  # Required because libnvinfer.so.7 depends on libnvrtc.so.11.1
  rm TensorRT-*
fi


#
# LINKER HACK!
#
if [ ! -f /usr/bin/x86_64-linux-gnu-ld.orig ]; then
  mv /usr/bin/x86_64-linux-gnu-ld     /usr/bin/x86_64-linux-gnu-ld.orig
fi
if [   -f /usr/bin/x86_64-linux-gnu-ld.lua ]; then
  mv /usr/bin/x86_64-linux-gnu-ld.lua /usr/bin/x86_64-linux-gnu-ld
fi


#
# LZ4
#
LZ4_VERSION=1.9.3
LZ4_PREFIX="/cvmfs/ai.mila.quebec/apps/x86_64/common/lz4"
if [ ! -d "$LZ4_PREFIX/$LZ4_VERSION" ]; then
  rm -Rf lz4
  git clone 'https://github.com/lz4/lz4.git' && cd lz4
  git checkout v$LZ4_VERSION
  make install -j$NJ MOREFLAGS="-mtune=haswell -Wl,-z,now" \
                     PREFIX="$LZ4_PREFIX/$LZ4_VERSION"
  cd ..
  rm -Rf lz4
fi


#
# LZMA/XZ-Utils
#
LZMA_VERSION=5.2.5
LZMA_PREFIX="/cvmfs/ai.mila.quebec/apps/x86_64/common/lzma"
if [ ! -d "$LZMA_PREFIX/$LZMA_VERSION" ]; then
  rm  -Rf xz-${LZMA_VERSION}.tar.gz xz-${LZMA_VERSION}
  wget "https://tukaani.org/xz/xz-${LZMA_VERSION}.tar.gz"
  tar -xf xz-${LZMA_VERSION}.tar.gz && cd xz-${LZMA_VERSION}
  ./configure --prefix="$LZMA_PREFIX/$LZMA_VERSION"
  make install -j$NJ
  cd  ..
  rm  -Rf xz-${LZMA_VERSION}.tar.gz xz-${LZMA_VERSION}
fi


#
# Zstd
#
ZSTD_VERSION=1.5.0
ZSTD_PREFIX="/cvmfs/ai.mila.quebec/apps/x86_64/common/zstd"
if [ ! -d "$ZSTD_PREFIX/$ZSTD_VERSION" ]; then
  rm -Rf zstd
  git clone 'https://github.com/facebook/zstd.git' && cd zstd
  git checkout v${ZSTD_VERSION}
  env LIBRARY_PATH="$LZ4_PREFIX/$LZ4_VERSION/lib:$LZMA_PREFIX/$LZMA_VERSION/lib" \
      CPATH="$LZ4_PREFIX/$LZ4_VERSION/include:$LZMA_PREFIX/$LZMA_VERSION/include" \
      make install -j$NJ MOREFLAGS="-mtune=haswell -Wl,-z,now" \
                         PREFIX="$ZSTD_PREFIX/$ZSTD_VERSION"
  cd ..
  rm -Rf zstd
fi


#
# Py-LMDB
#
# py-lmdb uses a patched, statically-linked version of LMDB, but generally a
# py-lmdb version bundles a specific matching LMDB version.
#
# Very handily, the installed source code for py-lmdb is exactly identical
# across all Python versions except for the native C extensions, which all
# happen to have a versioned ABI tag. Therefore, it is possible to install
# py-lmdb for all Python 3.x versions in "overlapped" fashion, thus having
# a common PYTHONPATH for each.
#
LMDB_VERSION=0.9.29
LMDB_PREFIX="/cvmfs/ai.mila.quebec/apps/x86_64/common/lmdb"
if [ ! -d "$LMDB_PREFIX/$LMDB_VERSION" ]; then
  rm -Rf lmdb
  git clone 'https://github.com/LMDB/lmdb.git' && cd lmdb/libraries/liblmdb
  git checkout LMDB_${LMDB_VERSION}
  make install -j$NJ XCFLAGS="-mtune=haswell -Wl,-z,now" \
                     prefix="$LMDB_PREFIX/$LMDB_VERSION"
  cd ../../..
  rm -Rf lmdb
fi

PYLMDB_VERSION=1.2.1
PYLMDB_PREFIX="/cvmfs/ai.mila.quebec/apps/x86_64/common/py-lmdb"
if [ ! -d "$PYLMDB_PREFIX/$PYLMDB_VERSION" ]; then
  rm -Rf "$PYLMDB_PREFIX/${PYLMDB_VERSION}_tmp" py-lmdb
  git clone 'https://github.com/jnwatson/py-lmdb.git' && cd py-lmdb
  git checkout py-lmdb_${PYLMDB_VERSION}
  for PYV in 3.6 3.7 3.8; do
    python${PYV} -m venv venv${PYV}
    (
      . venv${PYV}/bin/activate
      python -m pip install wheel Cython
      env -u LMDB_FORCE_CFFI   \
          -u LMDB_FORCE_SYSTEM \
          -u LMDB_PURE         \
          -u LMDB_INCLUDEDIR   \
          -u LMDB_LIBDIR       \
          CFLAGS=" -mtune=haswell -Wl,-z,now" \
          LDFLAGS="-mtune=haswell -Wl,-z,now" \
          python setup.py bdist_wheel
    )
    mkdir -p  "$PYLMDB_PREFIX/${PYLMDB_VERSION}_tmp"
    unzip -od "$PYLMDB_PREFIX/${PYLMDB_VERSION}_tmp" dist/lmdb-*.whl
    git clean -xdff
  done
  cd ..
  rm -Rf py-lmdb
  mv "$PYLMDB_PREFIX/${PYLMDB_VERSION}_tmp" "$PYLMDB_PREFIX/${PYLMDB_VERSION}"
fi


#
# OpenBLAS
# The target list on x86_64 is
#     SSE2:     GENERIC      (x86-64-baseline)
#     SSE3:     Prescott
#     SSE4.2:   Nehalem      (x86-64-v2)
#     AVX:      Sandy Bridge
#     AVX2+FMA: Haswell      (x86-64-v3)
#     AVX512:   Skylake-X    (x86-64-v4)
#     AMD AVX2: Zen2         (x86-64-v3, ~Haswell)
# We configure with BIGNUMA=1 because we're already at the 256-core limit with
# the biggest machines.
#
OPENBLAS_VERSION=0.3.17
OPENBLAS_PREFIX="/cvmfs/ai.mila.quebec/apps/x86_64/common/openblas"
if [ ! -d "$OPENBLAS_PREFIX/$OPENBLAS_VERSION" ]; then
  git clone 'https://github.com/xianyi/OpenBLAS.git' && cd OpenBLAS
  git checkout v${OPENBLAS_VERSION}
  make -j$NJ   USE_THREAD=1 USE_OPENMP=0 BIGNUMA=1 NUM_THREADS=256 BUILD_BFLOAT16=1 TARGET=GENERIC \
               DYNAMIC_ARCH=1 DYNAMIC_LIST="PRESCOTT NEHALEM SANDYBRIDGE HASWELL SKYLAKEX ZEN"
  make install PREFIX="$OPENBLAS_PREFIX/$OPENBLAS_VERSION"
  rmdir "$OPENBLAS_PREFIX/$OPENBLAS_VERSION/bin" || true  # Apparently empty.
  cd ..
  rm -Rf OpenBLAS
fi


#
# Numpy
# We link against OpenBLAS, and build for the following combinations:
#   - v1.19.5: py3.6+
#   - v1.20.3: py3.7+
#   - v1.21.2: py3.7+
# Numpy's setup.py seems to ignore CPATH and LIBRARY_PATH; It seems to only
# care about
#   - OPENBLAS (for OpenBLAS) or
#   - MKLROOT  (for MKL)
# to find the location of the library.
#
NPY_PREFIX="/cvmfs/ai.mila.quebec/apps/x86_64/common/numpy"
export OPENBLAS="$OPENBLAS_PREFIX/$OPENBLAS_VERSION"
export NPY_BLAS_ORDER="OpenBLAS"
export NPY_LAPACK_ORDER="OpenBLAS"
export NPY_NUM_BUILD_JOBS=$NJ
for NPY in 1.19.5 1.20.3 1.21.2; do
  if [ "$NPY" = 1.19.5 ]; then PYV_LIST="3.6 3.7 3.8" CPU_BASELINE=''
  else                         PYV_LIST="    3.7 3.8" CPU_BASELINE='--cpu-baseline=sse sse2'
  fi
  for PYV in $PYV_LIST; do
    if [ ! -d "${NPY_PREFIX}/python${PYV}-v${NPY}" ]; then
      if [ ! -d Numpy ]; then git clone https://github.com/numpy/numpy.git Numpy; fi
      cp -a Numpy "Numpy-python${PYV}-v${NPY}"
      (
        cd        "Numpy-python${PYV}-v${NPY}"
        git checkout v$NPY
        python${PYV} -m venv venv
        . venv/bin/activate
        python -m pip install wheel Cython
        python setup.py build ${CPU_BASELINE:+"$CPU_BASELINE"} bdist_wheel
        deactivate
        mkdir -p "${NPY_PREFIX}/python${PYV}-v${NPY}"
        unzip -d "${NPY_PREFIX}/python${PYV}-v${NPY}" dist/numpy-*.whl
        cd ..
        rm -Rf    "Numpy-python${PYV}-v${NPY}"
      ) &
    fi
  done
done
wait
rm -Rf Numpy Numpy-*
NPY_OLD=1.19.5   # Used as compatibility target in further recipes!


#
# Scipy
# The BLAS configuration uses the same mechanism as Numpy, but Scipy also
# depends on a package called Pythran.
#
# We deliberately build against the oldest Numpy from above.
#
SPY_PREFIX="/cvmfs/ai.mila.quebec/apps/x86_64/common/scipy"
git clone https://github.com/scipy/scipy.git Scipy
for SPY in 1.5.4 1.6.3 1.7.1; do
  if [ "$SPY" = 1.5.4 ]; then PYV_LIST="3.6 3.7 3.8"
  else                        PYV_LIST="    3.7 3.8"
  fi
  for PYV in $PYV_LIST; do
    if [ ! -d "${SPY_PREFIX}/python${PYV}-v${SPY}" ]; then
      if [ ! -d Scipy ]; then git clone https://github.com/scipy/scipy.git Scipy; fi
      cp -a Scipy "Scipy-python${PYV}-v${SPY}"
      (
        cd        "Scipy-python${PYV}-v${SPY}"
        git checkout v$SPY
        git submodule sync
        git submodule update --init
        python${PYV} -m venv venv
        . venv/bin/activate
        export PYTHONPATH="${NPY_PREFIX}/python${PYV}-v${NPY_OLD}:${PYTHONPATH}"
        python -m pip install wheel Cython pythran pybind11
        python setup.py bdist_wheel
        deactivate
        mkdir -p "${SPY_PREFIX}/python${PYV}-v${SPY}"
        unzip -d "${SPY_PREFIX}/python${PYV}-v${SPY}" dist/scipy-*.whl
        cd ..
        rm -Rf    "Scipy-python${PYV}-v${SPY}"
      ) &
    fi
  done
done
wait
rm -Rf Scipy Scipy-*
SPY_OLD=1.5.4   # Used as compatibility target in further recipes!


#
# FFTW
#
# The build is done twice - in double precision (--disable-single) and
# single precision (--enable-single). FFTW installs itself with different
# names in both modes, but uses the same header.
#
# In both cases, both static (--enable-static) and shared (--enable-shared)
# libraries are built and installed.
#
# The OpenMP build is disabled because libgomp (the standard OpenMP runtime
# shipping with GCC), once initialized, interacts very badly with fork(), a
# common operation in Python to spawn worker processes.
#
# AVX-512 is considered experimental. We disable the optimization for now,
# even though it might be of great benefit on Intel Skylake-X with AVX-512,
# but this should be reconsidered as soon as FFTW's authors deem it stable.
#
# We disable KCVI (Knights Corner Vector Instructions) because Intel KC
# hardware is obsolete.
#
FFTW_VERSION=3.3.9
FFTW_PREFIX="/cvmfs/ai.mila.quebec/apps/x86_64/common/fftw"
if [ ! -d "$FFTW_PREFIX/$FFTW_VERSION" ]; then
  rm  -Rf fftw-${FFTW_VERSION}.tar.gz fftw-${FFTW_VERSION}
  wget "https://www.fftw.org/fftw-${FFTW_VERSION}.tar.gz"
  tar -xf fftw-${FFTW_VERSION}.tar.gz && cd fftw-${FFTW_VERSION}
  for ENABLE_PRECISION in --disable-single --enable-single; do
    (
      export OMPI_ALLOW_RUN_AS_ROOT=1           # make check runs tests with mpirun.
      export OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1   # make check runs tests with mpirun.
      export PATH="/usr/mpi/gcc/openmpi-4.1.0rc5/bin:$PATH"
      ./configure --enable-static      \
                  --enable-shared      \
                  --disable-openmp     \
                  --enable-threads     \
                  --enable-mpi         \
                  --enable-sse2        \
                  --enable-avx         \
                  --enable-avx2        \
                  --enable-fma         \
                  --enable-avx-128-fma \
                  --disable-avx512     \
                  --disable-kcvi       \
                  ${ENABLE_PRECISION}  \
                  --prefix="$FFTW_PREFIX/$FFTW_VERSION"
      make -j$NJ
      make check          # FFTW has been known to break with some compilers. Best to be safe.
      make install
      make distclean
    )
  done
  cd  ..
  rm  -Rf fftw-${FFTW_VERSION}.tar.gz fftw-${FFTW_VERSION}
fi


#
# CUDA-related
# We support:
#   - Kepler  (K80 only, legacy)
#   - Maxwell (M40 only, legacy)
#   - Volta   (V100)
#   - Turing  (RTX8000, TITAN RTX)
# And on CUDA Toolkits where they are supported,
#   - Ampere  (A100, A6000)
#

#
# NCCL
#
NCCL_VERSION_BUILD=2.10.3-1
NCCL_VERSION=${NCCL_VERSION_BUILD%%-*}   # Strip build number
NCCL_VERSION=${NCCL_VERSION%.*}          # Strip patch number
NCCL_PREFIX="/cvmfs/ai.mila.quebec/apps/x86_64/common/nccl"
for CUDA in 10.2 11.0 11.1 11.2 11.3 11.4; do
  case "$CUDA" in
    10.2 )
      NVCC_GENCODE="-gencode=arch=compute_37,code=sm_37 -gencode=arch=compute_52,code=sm_52 \
                    -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 \
                    -Wno-deprecated-gpu-targets -Xfatbin -compress-all"
      ;;
    11.0 )
      NVCC_GENCODE="-gencode=arch=compute_37,code=sm_37 -gencode=arch=compute_52,code=sm_52 \
                    -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 \
                    -gencode=arch=compute_80,code=sm_80 \
                    -Wno-deprecated-gpu-targets -Xfatbin -compress-all"
      ;;
    11.[^0] )
      NVCC_GENCODE="-gencode=arch=compute_37,code=sm_37 -gencode=arch=compute_52,code=sm_52 \
                    -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 \
                    -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 \
                    -Wno-deprecated-gpu-targets -Xfatbin -compress-all"
      ;;
  esac
  if [ ! -d "$NCCL_PREFIX/$CUDA-v$NCCL_VERSION" ]; then
    if [ ! -d NCCL ]; then git clone https://github.com/NVIDIA/nccl.git NCCL; fi
    (
      cd NCCL
      git checkout v$NCCL_VERSION_BUILD
      export PATH="$TK/$CUDA/bin:${PATH}"
      export CPATH="$TK/$CUDA/include"
      export LIBRARY_PATH="$TK/$CUDA/lib64"
      export LDFLAGS="-Wl,-z,now,-rpath,$TK/$CUDA/lib64"
      make -j$NJ src.build CUDA_HOME="$TK/$CUDA" NVCC_GENCODE="$NVCC_GENCODE"
      make install PREFIX="$NCCL_PREFIX/$CUDA-v$NCCL_VERSION"
      make clean
      git clean -xdff
    )
  fi
done
rm -Rf NCCL


#
# MAGMA
#
MAGMA_VERSION=2.6.1
MAGMA_PREFIX="/cvmfs/ai.mila.quebec/apps/x86_64/common/magma"
for CUDA in 10.2 11.0 11.1 11.2 11.3 11.4; do
  case "$CUDA" in
    10.2 )     GPU_TARGET="sm_37 sm_52 sm_70 sm_75" ;;
    11.0 )     GPU_TARGET="sm_37 sm_52 sm_70 sm_75 sm_80" ;;
    11.[^0] )  GPU_TARGET="sm_37 sm_52 sm_70 sm_75 sm_80 sm_86" ;;
  esac
  if [ ! -d "$MAGMA_PREFIX/$CUDA-v$MAGMA_VERSION" ]; then
    if [ ! -d MAGMA ]; then git clone https://bitbucket.org/icl/magma.git MAGMA; fi
    cd MAGMA
    git checkout v${MAGMA_VERSION}
    #
    # Lightly edit the example OpenBLAS makefile as it is copied to make it more usable:
    #   - Enable overriding GPU_TARGET
    #   - Disable OpenMP (bad forking with GNU OMP libgomp)
    #
    sed -e 's/GPU_TARGET\s*=/GPU_TARGET ?=/g' \
        -e 's/-fopenmp//g' \
        make.inc-examples/make.inc.openblas > make.inc
    env BACKEND=cuda \
        PATH="$TK/$CUDA/bin:${PATH}" \
        CUDADIR="$TK/$CUDA" \
        LIBDIR="-L$TK/$CUDA/lib64" \
        OPENBLASDIR="$OPENBLAS_PREFIX/$OPENBLAS_VERSION" \
        GPU_TARGET="$GPU_TARGET" \
        NVCCFLAGS="-Wno-deprecated-gpu-targets -Xfatbin -compress-all" \
        make install -j$NJ prefix="$MAGMA_PREFIX/$CUDA-v$MAGMA_VERSION"
    git clean -xdff
    cd ..
  fi
done
rm -Rf MAGMA


#
# PyTorch
#
PYTORCH_PREFIX="/cvmfs/ai.mila.quebec/apps/x86_64/debian/pytorch"
rm -Rf PyTorch PyTorch-* "$PYTORCH_PREFIX" log-*.txt
git clone --recursive https://github.com/pytorch/pytorch.git PyTorch
for PYTORCH_VERSION in 1.8.2 1.9.0; do
  cd PyTorch
  git clean -xdff
  git checkout v${PYTORCH_VERSION}
  git submodule sync --recursive
  git submodule update --init --recursive
  git submodule sync --recursive
  git submodule update --init --recursive
  git clean -xdff
  cd ..
  for CUDA in 10.2 11.0 11.1 11.2; do
    case "$CUDA" in
      10.2 )     TORCH_CUDA_ARCH_LIST="3.7;5.2;7.0;7.5" ;;
      11.0 )     TORCH_CUDA_ARCH_LIST="3.7;5.2;7.0;7.5;8.0" ;;
      11.[^0] )  TORCH_CUDA_ARCH_LIST="3.7;5.2;7.0;7.5;8.0;8.6" ;;
    esac
    (
      rm -Rf        "PyTorch-cuda${CUDA}"
      cp -a PyTorch "PyTorch-cuda${CUDA}"
      cd            "PyTorch-cuda${CUDA}"
      for PYV in 3.6 3.7 3.8; do
        PYVS="$(printf '%s\n' "$PYV" | sed 's/\.//g')"
        (
          python${PYV} -m venv venv
          . venv/bin/activate
          
          export PYTHONPATH="${SPY_PREFIX}/python${PYV}-v${SPY_OLD}:${PYTHONPATH}"
          export PYTHONPATH="${NPY_PREFIX}/python${PYV}-v${NPY_OLD}:${PYTHONPATH}"
          
          export PYTORCH_BUILD_VERSION="${PYTORCH_VERSION}"
          export PYTORCH_BUILD_NUMBER=0
          export TORCH_NVCC_FLAGS="-Wno-deprecated-gpu-targets -Xfatbin -compress-all"
          export TORCH_CUDA_ARCH_LIST
          export BLAS=OpenBLAS             # Require OpenBLAS for linear algebra. Our OpenBLAS is built against pthreads and not OpenMP.
          export ATEN_THREADING=TBB        # Require TBB rather than OpenMP because OpenMP forks badly.
          export MKL_THREADING=TBB         # Require TBB rather than OpenMP because OpenMP forks badly.
          export MKLDNN_CPU_RUNTIME=TBB    # Require TBB rather than OpenMP because OpenMP forks badly.
          export MKLDNN_ARCH_OPT_FLAGS="-msse2"  # Require portable MKLDNN (SSE2, defaults to SSE4.1)
          export DNNL_ARCH_OPT_FLAGS="-msse2"    # Require portable MKLDNN (SSE2, defaults to SSE4.1)
          export USE_TBB=1                 # Require TBB
          export USE_OPENMP=0              # Demand not to use OpenMP (due to fork-unsafeness of libgomp, the default implementation of OpenMP).
          export USE_MKLDNN=1              # Demand to use MKLDNN
          export USE_CUDNN=1               # Demand to use cuDNN
          export USE_STATIC_CUDNN=0        # Want dynamic link to avoid wasting disk space
          export USE_SYSTEM_NCCL=1         # Compile using system NCCL
          export USE_STATIC_NCCL=0         # Want dynamic link to avoid wasting disk space
          export USE_DISTRIBUTED=1         # Want distributed backends enabled
          export USE_MPI=1                 # Want (Open)MPI  distributed backend
          export USE_GLOO=1                # Want Gloo       distributed backend
          export USE_TENSORPIPE=1          # Want TensorPipe distributed backend
          export USE_IBVERBS=1             # Want ibverbs
          export USE_ZSTD=1                # Want Zstd
          export USE_LMDB=1                # Want LMDB
          export USE_TENSORRT=1            # Want TensorRT
          export BUILD_CUSTOM_PROTOBUF=1   # Enable building custom protobuf.
          
          export CUDA_HOME="$TK/$CUDA"                                    # The standard way to signal the CUDA Toolkit's location.
          export CUDNN_ROOT="$CUDNN_PREFIX/${CUDA}-v${CUDNN_VERSION}"     # Wanted by FindCUDNN.cmake
          export NCCL_VERSION                                             # Wanted by FindNCCL.cmake, forces correct version check.
          export NCCL_ROOT="$NCCL_PREFIX/${CUDA}-v${NCCL_VERSION}"        # Wanted by FindNCCL.cmake
          export NCCL_LIB_DIR="${NCCL_ROOT}/lib"                          # Wanted by FindNCCL.cmake
          export NCCL_INCLUDE_DIR="${NCCL_ROOT}/include"                  # Wanted by FindNCCL.cmake
          export TENSORRT_ROOT="$TRT_PREFIX/cuda${CUDA}-cudnn${CUDNN_VERSION}-v${TRT_VERSION}"  # Wanted by cmake/public/cuda.cmake
          export TENSORRT_INCLUDE_DIR="${TENSORRT_ROOT}/include"          # Wanted by cmake/public/cuda.cmake
          export LMDB_DIR="$LMDB_PREFIX/$LMDB_VERSION"                    # Wanted by FindLMDB.cmake
          export MAGMA_HOME="$MAGMA_PREFIX/${CUDA}-v${MAGMA_VERSION}"     # Wanted by FindMAGMA.cmake
          # A small difficulty appears with PyTorch 1.9.0 re: OpenBLAS. PyTorch 1.9.0 performs
          # detection for BLAS twice, using different logic, and one path reuses $OpenBLAS_HOME
          # while the other uses a different strategy that involves looking at hardcoded paths,
          # $CPATH, $LIBRARY_PATH *and* $LD_LIBRARY_PATH, even though the latter variable
          # normally serves a different purpose. But we are obliged to use this later variable
          # because otherwise an -L flag is not supplied to the linker, and so our linker hack
          # to inject -rpath is not activated. So we therefore indicate the location of OpenBLAS
          # through all of $OpenBLAS_HOME, $CPATH and $LD_LIBRARY_PATH but *not* $LIBRARY_PATH,
          # however strange. This does not appear to hurt PyTorch 1.8.x.
          export OpenBLAS_HOME="$OPENBLAS_PREFIX/$OPENBLAS_VERSION"       # Wanted by FindOpenBLAS.cmake (PyTorch 1.8.x)
          export LD_LIBRARY_PATH="$OPENBLAS_PREFIX/$OPENBLAS_VERSION/lib:$LD_LIBRARY_PATH" # Wanted by FindBLAS.cmake     (PyTorch 1.9.x). We must not use LIBRARY_PATH in order to leverage the linker hack.
          export CPATH="$OPENBLAS_PREFIX/$OPENBLAS_VERSION/include"       # Wanted by FindBLAS.cmake     (PyTorch 1.9.x)
          export PATH="/usr/mpi/gcc/openmpi-4.1.0rc5/bin:$PATH"           # Sufficient to discover OpenMPI
          
          # Propagate additional variables into CMake's invocation
          sed -i -e "s:'MKLDNN_CPU_RUNTIME',:'MKLDNN_CPU_RUNTIME', 'MKLDNN_ARCH_OPT_FLAGS', 'DNNL_ARCH_OPT_FLAGS', 'TENSORRT_ROOT', 'SOURCE_DATE_EPOCH',:g" tools/setup_helpers/cmake.py
          
          # Install wheel first (becauses the requirements need it), then reqs, then build PyTorch.
          python -m pip install wheel
          python -m pip install -r requirements.txt
          LD_LUA_LOGNAME="/opt/log-python${PYV}-cuda${CUDA}-v${PYTORCH_VERSION}.txt" \
          python setup.py bdist_wheel
          
          # Repair patch to tools/setup_helpers/cmake.py
          git checkout -- tools/setup_helpers/cmake.py
          
          # Expand the wheel into the destination.
          mkdir -p "${PYTORCH_PREFIX}/python${PYV}-cuda${CUDA}-cudnn${CUDNN_VERSION}-v${PYTORCH_VERSION}"
          unzip -d "${PYTORCH_PREFIX}/python${PYV}-cuda${CUDA}-cudnn${CUDNN_VERSION}-v${PYTORCH_VERSION}" dist/torch-*.whl
          rm -Rf dist
          
          # Leave
          deactivate
          rm -Rf venv
        )
      done
      cd ..
      rm -Rf "PyTorch-cuda${CUDA}"
    ) &
  done
  wait
done
rm -Rf PyTorch


#
# Print compiler cache stats on exit, then clean out the cache.
#
ccache -s
ccache -C

